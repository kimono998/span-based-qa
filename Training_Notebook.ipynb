{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d70b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.21.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adefc05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, pipeline\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "\n",
    "class SquadDS(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "\n",
    "def unpack_dataset():\n",
    "    train_ds = load_dataset('squad', split='train')\n",
    "    test_ds = load_dataset('squad', split='validation')\n",
    "    df1 = pd.DataFrame(train_ds)\n",
    "    df2 = pd.DataFrame(test_ds)\n",
    "\n",
    "    faulty_df1 = [3275, 3276, 11308, 49094, 50763, 57586, 57587, 57588, 57589, 57590, 60024, 60027, 66282]\n",
    "\n",
    "    faulty_df2 = [4145, 4146, 4264, 4269, 4282, 4283, 4851, 4852, 4853]\n",
    "\n",
    "    df1 = df1.drop(faulty_df1)\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2 = df2.drop(faulty_df2)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    train_questions = df1.question.tolist()\n",
    "    test_questions = df2.question.tolist()\n",
    "\n",
    "    train_context = df1.context.tolist()\n",
    "    test_context = df2.context.tolist()\n",
    "\n",
    "    train_answers = df1.answers.tolist()\n",
    "    test_answers = df2.answers.tolist()\n",
    "\n",
    "    return train_questions, train_context, train_answers, test_questions, test_context, test_answers\n",
    "\n",
    "\n",
    "def add_answer_end(answers, context):\n",
    "    for answer, context in zip(answers, context):\n",
    "        if answer['answer_start'] is None:\n",
    "            answer['answer_end'] = None\n",
    "        else:\n",
    "            if len(answer['text']) > 1:\n",
    "                temp_end = []\n",
    "                temp_start = []\n",
    "                for ans, start in zip(answer['text'], answer['answer_start']):\n",
    "                    gold = ans\n",
    "                    end_idx = start + len(gold)\n",
    "                    if context[start:end_idx] == gold:\n",
    "                        temp_start.append(start)\n",
    "                        temp_end.append(end_idx)\n",
    "\n",
    "                    elif context[start - 1:end_idx - 1] == gold:\n",
    "                        temp_start.append(start - 1)\n",
    "                        temp_end.append(end_idx - 1)\n",
    "\n",
    "                    elif context[start - 2:end_idx - 2] == gold:\n",
    "                        temp_start.append(start - 2)\n",
    "                        temp_end.append(end_idx - 2)\n",
    "\n",
    "                answer['answer_end'] = temp_end\n",
    "                answer['answer_start'] = temp_start\n",
    "\n",
    "            else:\n",
    "                gold = answer['text'][0]\n",
    "                start_idx = answer['answer_start']\n",
    "                end_idx = start_idx[0] + len(gold)\n",
    "                if context[start_idx[0]:end_idx] == gold:\n",
    "                    answer['answer_end'] = [end_idx]\n",
    "                elif context[start_idx[0] - 1:end_idx - 1] == gold:\n",
    "                    answer['answer_start'] = [start_idx[0] - 1]\n",
    "                    answer['answer_end'] = [end_idx - 1]  # When the gold label is off by one character\n",
    "                elif context[start_idx[0] - 2:end_idx - 2] == gold:\n",
    "                    answer['answer_start'] = [start_idx[0] - 2]\n",
    "                    answer['answer_end'] = [end_idx - 2]  # When the gold label is off by two characters\n",
    "\n",
    "\n",
    "def define_token_position(encoding, answers):\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for idx in range(len(answers)):\n",
    "        if len(answers[idx]['answer_start']) > 1:\n",
    "            temp_st = []\n",
    "            temp_en = []\n",
    "            for ans_st in answers[idx]['answer_start']:\n",
    "                if ans_st is None:\n",
    "                    temp_st.append(tokenizer.model_max_length - 1)\n",
    "                else:\n",
    "                    temp_st.append(encoding.char_to_token(idx, ans_st))\n",
    "            for ans_en in answers[idx]['answer_end']:\n",
    "                if ans_en is None:\n",
    "                    temp_en.append(tokenizer.model_max_length - 1)\n",
    "                else:\n",
    "                    temp_en.append(encoding.char_to_token(idx, ans_en - 1))\n",
    "            start_positions.append(temp_st)\n",
    "            end_positions.append(temp_en)\n",
    "            # answers[idx]['answer_start'] = temp_st\n",
    "            # answers[idx]['answer_end'] = temp_en\n",
    "        else:\n",
    "\n",
    "            if answers[idx]['answer_start'] is None:\n",
    "                # answers[idx]['answer_start'] = tokenizer.model_max_length - 1\n",
    "                st = tokenizer.model_max_length - 1\n",
    "            else:\n",
    "                # answers[idx]['answer_start'] = encoding.char_to_token(idx, answers[idx]['answer_start'][0])\n",
    "                st = encoding.char_to_token(idx, answers[idx]['answer_start'][0])\n",
    "            if answers[idx]['answer_end'] is None:\n",
    "                # answers[idx]['answer_end'] = tokenizer.model_max_length - 1\n",
    "                en = tokenizer.model_max_length - 1\n",
    "            else:\n",
    "                # answers[idx]['answer_end'] = encoding.char_to_token(idx, answers[idx]['answer_end'][0] - 1)\n",
    "                en = encoding.char_to_token(idx, answers[idx]['answer_end'][0] - 1)\n",
    "            # if None, the answer passage has been truncated due to words > 512 so setting last position as 511\n",
    "            start_positions.append(st)\n",
    "            end_positions.append(en)\n",
    "\n",
    "    encoding.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "\n",
    "def save_metrics_json(loss_epoch,\n",
    "                      acc_epoch,\n",
    "                      f1_epoch,\n",
    "                      test_acc,\n",
    "                      avg_test_f1,\n",
    "                      file_path):\n",
    "    metrics = {}\n",
    "    metrics[\"train\"] = {}\n",
    "    metrics[\"test\"] = {}\n",
    "\n",
    "    for i in range(len(loss_epoch)):\n",
    "        metrics[\"train\"][f\"epoch{i + 1}\"] = {}\n",
    "        metrics[\"train\"][f\"epoch{i + 1}\"][\"loss\"] = loss_epoch[i]\n",
    "        metrics[\"train\"][f\"epoch{i + 1}\"][\"accuracy\"] = acc_epoch[i]\n",
    "        metrics[\"train\"][f\"epoch{i + 1}\"][\"f1_score\"] = f1_epoch[i]\n",
    "        metrics[\"test\"][\"accuracy\"] = test_acc\n",
    "        metrics[\"test\"][\"f1_score\"] = avg_test_f1\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(metrics, json_file, indent=4)\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    Load and return a RobertaForQuestionAnswering model with parameters from the file provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : path to the saved model parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : a RobertaForQuestionAnswering model with the parameters from the file provided.\n",
    "\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(path, map_location=torch.device('cuda:2'))\n",
    "\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\", state_dict=state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def f(questions, start):\n",
    "    outs = []\n",
    "    for q in questions:\n",
    "        question, context = q.split(\"[SEP]\")\n",
    "        d = pmodel.tokenizer(question, context)\n",
    "        out = pmodel.model.forward(**{k: torch.tensor(d[k]).reshape(1, -1) for k in d})\n",
    "        logits = out.start_logits if start else out.end_logits\n",
    "        outs.append(logits.reshape(-1).detach().numpy())\n",
    "    return outs\n",
    "\n",
    "\n",
    "#model = load_model('distillbert_done')\n",
    "# define two predictions, one that outputs the logits for the range start,\n",
    "#pmodel = pipeline(task='question-answering', model=model,\n",
    "#                  tokenizer=DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-distilled-squad'))\n",
    "\n",
    "def f_start(questions):\n",
    "    return f(questions, True)\n",
    "\n",
    "\n",
    "def f_end(questions):\n",
    "    return f(questions, False)\n",
    "\n",
    "\n",
    "def out_names(inputs):\n",
    "    question, context = inputs.split(\"[SEP]\")\n",
    "    d = pmodel.tokenizer(question, context)\n",
    "    return [pmodel.tokenizer.decode([id]) for id in d[\"input_ids\"]]\n",
    "\n",
    "#get's the output of true and false items for shap visualizations\n",
    "def shap_test(tt):\n",
    "    device = 'cuda:3'  # if torch.cuda.is_available()# else 'cpu'\n",
    "\n",
    "    # print('Cached:   ', round(torch.cuda.memory_reserved(torch.cuda.current_device)/1024**3,1), 'GB')\n",
    "    # print('Allocated:', round(torch.cuda.memory_allocated(torch.cuda.current_device)/1024**3,1), 'GB')\n",
    "\n",
    "    torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "    true = []\n",
    "    false = []\n",
    "    model.to(device)\n",
    "    device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "    input_ids = tt['input_ids'].to(device)\n",
    "    attention_mask = tt['attention_mask'].to(device)\n",
    "    start_positions = tt['start_positions'].to(device)\n",
    "    end_positions = tt['end_positions'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.start_logits\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "    # print(start_positions.shape[0])\n",
    "    if start_positions.numel() > 1:\n",
    "        for j in range(start_positions.shape[0]):\n",
    "            if pred == start_positions[j]:\n",
    "                return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f07313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH = 'distillbert_done'\n",
    "\n",
    "\n",
    "def train(model, train_dataset):\n",
    "    device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "    save_path = 'model_checkpoint_epoch{}.pt'\n",
    "    save_interval = 1\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_epoch = []\n",
    "    acc_epoch = []\n",
    "    f1_epoch = []\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(6):\n",
    "\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(torch.cuda.current_device) / 1024 ** 3, 1), 'GB')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(torch.cuda.current_device) / 1024 ** 3, 1), 'GB')\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_pred = 0\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        model.train()\n",
    "\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "\n",
    "            logits = outputs.start_logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            total_correct += torch.sum(pred == start_positions).item()\n",
    "            total_pred += len(start_positions)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            true_labels.extend(start_positions.tolist())\n",
    "            predicted_labels.extend(pred.tolist())\n",
    "            avg_train_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        avg_train_acc = total_correct / total_pred\n",
    "        acc_epoch.append(avg_train_acc)\n",
    "        loss_epoch.append(avg_train_loss)\n",
    "        f1_epoch.append(avg_train_f1)\n",
    "        print(\"average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"average training accuracy: {0:.2f}\".format(avg_train_acc))\n",
    "        print(\"average training f1_score: {0:.2f}\".format(avg_train_f1))\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            torch.save(model.state_dict(), save_path.format(epoch + 1))\n",
    "\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    return model, loss_epoch, f1_epoch, acc_epoch\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    start_positions = [item['start_positions'] for item in batch]\n",
    "    end_positions = [item['end_positions'] for item in batch]\n",
    "\n",
    "    input_ids_padded = pad_sequence([x.unsqueeze(0) if x.ndim == 0 else x for x in input_ids], batch_first=True,\n",
    "                                    padding_value=0)\n",
    "    attention_mask_padded = pad_sequence([x.unsqueeze(0) if x.ndim == 0 else x for x in attention_mask],\n",
    "                                         batch_first=True, padding_value=0)\n",
    "    start_positions_padded = pad_sequence([x.unsqueeze(0) if x.ndim == 0 else x for x in start_positions],\n",
    "                                          batch_first=True, padding_value=-1)\n",
    "    end_positions_padded = pad_sequence([x.unsqueeze(0) if x.ndim == 0 else x for x in end_positions], batch_first=True,\n",
    "                                        padding_value=-1)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'start_positions': start_positions_padded,\n",
    "        'end_positions': end_positions_padded\n",
    "    }\n",
    "\n",
    "\n",
    "def test(model, test_dataset):\n",
    "    device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "    total_correct = 0\n",
    "    total_pred = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.start_logits\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            for j in range(len(start_positions[i])):\n",
    "                if pred[i] == start_positions[i][j]:\n",
    "                    total_correct += 1\n",
    "                    break\n",
    "\n",
    "        total_pred += len(start_positions)\n",
    "\n",
    "        true_labels.extend(start_positions[:, 2].tolist())\n",
    "        predicted_labels.extend(pred.tolist())\n",
    "        avg_test_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    test_acc = total_correct / total_pred\n",
    "    print(\"average testing accuracy: {0:.2f}\".format(test_acc))\n",
    "    print(\"average testing f1_score: {0:.2f}\".format(avg_test_f1))\n",
    "\n",
    "    return avg_test_f1, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22415d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset squad (/home/users/dristic/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Found cached dataset squad (/home/users/dristic/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "/home/users/dristic/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached:    0.0 GB\n",
      "Allocated: 0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4380/10949 [26:26<42:17,  2.59it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 29%|██▊       | 3138/10949 [18:36<48:01,  2.71it/s]  ]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 10949/10949 [1:12:34<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 0.98\n",
      "average training accuracy: 0.69\n",
      "average training f1_score: 0.59\n",
      "Cached:    0.0 GB\n",
      "Allocated: 0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10949/10949 [1:12:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 0.73\n",
      "average training accuracy: 0.75\n",
      "average training f1_score: 0.67\n",
      "Cached:    0.0 GB\n",
      "Allocated: 0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10949/10949 [1:12:35<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 0.57\n",
      "average training accuracy: 0.80\n",
      "average training f1_score: 0.72\n",
      "Cached:    0.0 GB\n",
      "Allocated: 0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 5142/10949 [31:26<41:22,  2.34it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 10949/10949 [1:12:34<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 0.46\n",
      "average training accuracy: 0.84\n",
      "average training f1_score: 0.78\n",
      "Cached:    0.0 GB\n",
      "Allocated: 0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5503/10949 [33:56<36:01,  2.52it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 98%|█████████▊| 10710/10949 [1:10:50<01:47,  2.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "train_questions, train_context, train_answers, test_questions, test_context, test_answers = unpack_dataset()\n",
    "add_answer_end(train_answers, train_context)\n",
    "add_answer_end(test_answers, test_context)\n",
    "\n",
    "train_encodings = tokenizer(train_context, train_questions, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_context, test_questions, truncation=True, padding=True)\n",
    "\n",
    "define_token_position(train_encodings, train_answers)\n",
    "define_token_position(test_encodings, test_answers)\n",
    "\n",
    "# -----------#continue training#------------\n",
    "#saved_state_dict = torch.load('distillbert_done')\n",
    "#model.load_state_dict(saved_state_dict)\n",
    "# -----------###################------------\n",
    "\n",
    "train_dataset = SquadDS(train_encodings)\n",
    "test_dataset = SquadDS(test_encodings)\n",
    "model, loss_epoch, f1_epoch, acc_epoch = train(model, train_dataset)\n",
    "avg_test_f1, test_acc = test(model, test_dataset)\n",
    "save_metrics_json(loss_epoch, acc_epoch, f1_epoch, test_acc, avg_test_f1, 'metrics_distillbert_uncased.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed34ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset squad (/home/users/dristic/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Found cached dataset squad (/home/users/dristic/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      " 59%|█████▉    | 392/661 [01:16<00:52,  5.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m define_token_position(test_encodings, test_answers)\n\u001b[1;32m     18\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SquadDS(test_encodings)\n\u001b[0;32m---> 19\u001b[0m avg_test_f1, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_dataset)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pred)):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(start_positions[i])):\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pred[i] \u001b[38;5;241m==\u001b[39m start_positions[i][j]:\n\u001b[1;32m    138\u001b[0m             total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###test###\n",
    "\n",
    "\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer=DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "#--------/Load the model/--------#\n",
    "state_dict = torch.load('distillbert_done.pt')#, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "#--------/-------------/--------#\n",
    "\n",
    "_, _, _, test_questions, test_context, test_answers = unpack_dataset()\n",
    "add_answer_end(test_answers, test_context)\n",
    "test_encodings = tokenizer(test_context, test_questions, truncation=True, padding=True)\n",
    "define_token_position(test_encodings, test_answers)\n",
    "test_dataset = SquadDS(test_encodings)\n",
    "avg_test_f1, test_acc = test(model,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b599a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[1;32m      3\u001b[0m saved_state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistillbert_done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(saved_state_dict)\n\u001b[1;32m      5\u001b[0m avg_test_f1, test_acc \u001b[38;5;241m=\u001b[39m test(model,test_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "saved_state_dict = torch.load('distillbert_done')\n",
    "model.load_state_dict(saved_state_dict)\n",
    "avg_test_f1, test_acc = test(model,test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce10991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_json(loss_epoch, acc_epoch, f1_epoch, test_acc, avg_test_f1, 'metrics1.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff79b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast,pipeline\n",
    "import numpy as np\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "np.bool_ = bool\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    Load and return a RobertaForQuestionAnswering model with parameters from the file provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : path to the saved model parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : a RobertaForQuestionAnswering model with the parameters from the file provided.\n",
    "\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\", state_dict = state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "# load the model\n",
    "\n",
    "#model = load_model('saved_model_complete')\n",
    "#tokenizer=DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')# define two predictions, one that outputs the logits for the range start,\n",
    "\n",
    "#pmodel = pipeline(task='question-answering', model=model, tokenizer=DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad'))\n",
    "# and the other for the range end\n",
    "def f(questions, start):\n",
    "    outs = []\n",
    "    for q in questions:\n",
    "        question, context = q.split(\"[SEP]\")\n",
    "        d = pmodel.tokenizer(question, context)\n",
    "        out = pmodel.model.forward(**{k: torch.tensor(d[k]).reshape(1, -1) for k in d})\n",
    "        logits = out.start_logits if start else out.end_logits\n",
    "        outs.append(logits.reshape(-1).detach().numpy())\n",
    "    return outs\n",
    "def f_start(questions):\n",
    "    return f(questions, True)\n",
    "def f_end(questions):\n",
    "    return f(questions, False)\n",
    "\n",
    "# attach a dynamic output_names property to the models so we can plot the tokens at each output position\n",
    "def out_names(inputs):\n",
    "    question, context = inputs.split(\"[SEP]\")\n",
    "    d = pmodel.tokenizer(question, context)\n",
    "    return [pmodel.tokenizer.decode([id]) for id in d[\"input_ids\"]]\n",
    "f_start.output_names = out_names\n",
    "f_end.output_names = out_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
